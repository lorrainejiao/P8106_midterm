---
title: "P8106 Midterm Project Code"
author: "Xiaoluo Jiao"
date: "3/16/2022"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(caret)
library(AppliedPredictiveModeling)
library(glmnet)
library(mgcv)
library(MASS)
library(pROC)
library(vip)

knitr::opts_chunk$set(
  fig.width = 6, 
  fig.asp = .6,
  out.width = "90%")

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Prepare the data

```{r}
# Prepare the data
# Data source: https://www.kaggle.com/vishalyo990/prediction-of-quality-of-wine/data 
data = read_csv("winequality-red.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    quality = case_when(
      quality < 7 ~ "not_good", 
      quality >= 7 ~ "good"
    ), 
    quality = as.factor(quality), 
    quality = fct_relevel(quality, c("not_good", "good"))
  ) 

# partition
set.seed(105)
indexTrain <- createDataPartition(y = data$quality, p = 0.7, list = FALSE)
train <- data[indexTrain, ] 
test <- data[-indexTrain, ]

train_x = data.matrix(train %>% dplyr::select(-quality))
train_y = train$quality

test_x = data.matrix(test %>% dplyr::select(-quality))
test_y = test$quality
```

# Exploratory analysis

```{r}
# numerical summary
summary(data)

# graphical summary
theme1 <- transparentTheme(trans = .4)
trellis.par.set(theme1)

featurePlot(x = data[ , c(1:11)], 
            y = data$quality,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2)
            )
```

# Models
## Logistic

```{r glm}
# Logistic regression
set.seed(105)
glm.fit = glm(quality ~ ., 
              data = data, 
              subset = indexTrain, 
              family = binomial(link = "logit"))
summary(glm.fit)

# using caret
ctrl1 <- trainControl(method = "repeatedcv", repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

model.glm = train(train_x,
                  train_y,
                  method = "glm",
                  metric = "ROC",
                  trControl = ctrl1)
summary(model.glm)

#vip(model.glm$finalModel)
```

## GAM

```{r gam}
set.seed(105)
model.gam <- train(train_x,
                   train_y,
                   method = "gam",
                   metric = "ROC",
                   trControl = ctrl1)
model.gam$finalModel

# partial dependence plot
summary(model.gam)
# plot(model.gam$finalModel, page = 1)
```

## MARS

```{r mars}
set.seed(105)
mars_grid <- expand.grid(degree = 1:3, nprune = 2:25)

mars.fit <- train(train_x, train_y,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1)
summary(mars.fit)
ggplot(mars.fit)
mars.fit$bestTune

#important variables
# vip(mars.fit$finalModel)
```

## Penalized logistic

```{r}
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 6),
                        .lambda = exp(seq(-8, -1, length = 20)))
set.seed(105)
model.glmn <- train(train_x, 
                    train_y,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl1)
model.glmn$bestTune
summary(model.glmn)

# important variable
# vip(model.glmn$finalModel)
```

## LDA

```{r}
set.seed(105)
lda.fit <- lda(quality ~ ., 
               data = data,
               subset = indexTrain)
# plot(lda.fit)
summary(lda.fit)

#using caret 
set.seed(105)
model.lda <- train(train_x,
                   train_y,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl1)
```

# Model Selection

```{r}
res <- resamples(list(glm = model.glm,
                      glmn = model.glmn,
                      gam = model.gam,
                      mars = mars.fit,
                      lda = model.lda), 
                 metric = c("ROC"))
summary(res)
bwplot(res, metric = c("ROC"))
```

# ROC curve

```{r}
glm.pred <- predict(model.glm, newdata = test, type = "prob")[,2]
glmn.pred <- predict(model.glmn, newdata = test, type = "prob")[,2]
gam.pred <- predict(model.gam, newdata = test, type = "prob")[,2]
mars.pred <- predict(mars.fit, newdata = test, type = "prob")[,2]
lda.pred <- predict(model.lda, newdata = test, type = "prob")[,2]

roc.glm <- roc(test_y, glm.pred)
roc.glmn <- roc(test_y, glmn.pred)
roc.gam <- roc(test_y, gam.pred)
roc.mars <- roc(test_y, mars.pred)
roc.lda <- roc(test_y, lda.pred)

# AUC
auc <- c(roc.glm$auc[1], roc.glmn$auc[1], roc.gam$auc[1], roc.mars$auc[1], roc.lda$auc[1])

# ROC curves
modelNames <- c("glm", "glmn", "gam", "mars", "lda")

ggroc(list(roc.glm, roc.glmn, roc.gam, roc.mars, roc.lda), legacy.axes = TRUE) +
scale_color_discrete(labels = paste0(modelNames, " (", round(auc,3),")"),
name = "Models (AUC)") +
geom_abline(intercept = 0, slope = 1, color = "grey")
```


